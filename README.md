# LLM Evaluation
# Compendium of LLM Evaluation methods

---
### Reviews and Surveys

### Leaderboards and Arenas
#### LMSys Arena https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
#### MTEB https://huggingface.co/spaces/mteb/leaderboard
#### SWE Bench https://www.swebench.com/
#### Gorilla, Berkeley function calling Leaderboard https://gorilla.cs.berkeley.edu/leaderboard.html https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
#### WildBench WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild https://huggingface.co/spaces/allenai/WildBench
#### Enterprise Scenarios, Patronus https://huggingface.co/blog/leaderboard-patronus
#### Vectara Halluzinations https://github.com/vectara/hallucination-leaderboard

### Evaluation Software
#### MTEB https://huggingface.co/spaces/mteb/leaderboard
#### OpenICL Framework https://arxiv.org/abs/2303.02913
#### RAGAS https://docs.ragas.io/en/stable/	
﻿#### EleutherAI LLM Evaluation Harness https://github.com/EleutherAI/lm-evaluation-harness
﻿####  OpenAI Evals https://github.com/openai/evals
﻿####  ML Flow Evaluate https://mlflow.org/docs/latest/llms/llm-evaluate/index.html
﻿####  MosaicML Composer https://github.com/mosaicml/composer
﻿####  TruLens https://github.com/truera/trulens/
﻿#### BigCode Evaluation Harness https://github.com/bigcode-project/bigcode-evaluation-harness

### LLM Evaluation articles in tech media and blog posts from companies

### Large benchmarks

### Evaluation theory, evaluation methods
### LLM as Judge
---
## LLM Evaluation
### Embeddings
### In Context Learning
### Hallucinations
### Multi Turn
### Multi-Lingual
### Multi-Modal
### Instruction Following
### Ethical AI
### Safe AI
### Code Generating LLMs


---

## LLM Systems
### RAG Evaluation
### Conversational systems
### Copilots
### Search and Recommendation Engines


