# LLM Evaluation
# Compendium of LLM Evaluation methods

---
### Reviews and Surveys

### Leaderboards and Arenas
#### LMSys Arena https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
#### MTEB https://huggingface.co/spaces/mteb/leaderboard
#### SWE Bench https://www.swebench.com/
#### Gorilla, Berkeley function calling Leaderboard https://gorilla.cs.berkeley.edu/leaderboard.html https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
#### WildBench WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild https://huggingface.co/spaces/allenai/WildBench
#### Enterprise Scenarios, Patronus https://huggingface.co/blog/leaderboard-patronus
#### Vectara Halluzinations https://github.com/vectara/hallucination-leaderboard

### Evaluation Software
#### MTEB https://huggingface.co/spaces/mteb/leaderboard
#### OpenICL Framework https://arxiv.org/abs/2303.02913
#### RAGAS https://docs.ragas.io/en/stable/
#### EleutherAI LLM Evaluation Harness https://github.com/EleutherAI/lm-evaluation-harness
#### OpenAI Evals https://github.com/openai/evals
#### ML Flow Evaluate https://mlflow.org/docs/latest/llms/llm-evaluate/index.html
#### MosaicML Composer https://github.com/mosaicml/composer
#### TruLens https://github.com/truera/trulens/
#### BigCode Evaluation Harness https://github.com/bigcode-project/bigcode-evaluation-harness
ï»¿
### LLM Evaluation articles in tech media and blog posts from companies
#### https://techcrunch-com.cdn.ampproject.org/c/s/techcrunch.com/2024/03/23/why-its-impossible-to-review-ais-and-why-techcrunch-is-doing-it-anyway/amp/
#### https://blog.mozilla.ai/exploring-llm-evaluation-at-scale-with-the-neurips-large-language-model-efficiency-challenge/
####
#### 

### Large benchmarks

### Specific benchmarks
#### OpenEQA: From word models to world models, Meta, Apr 2024, Understanding physical spaces by Models,  https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/?utm_source=twitter&utm_medium=organic_social&utm_content=video&utm_campaign=dataset
#### ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models, Apr 2024, https://arxiv.org/pdf/2403.20262.pdf

### Evaluation theory, evaluation methods analysis of evaluation
#### Are We on the Right Way for Evaluating Large Vision-Language Models?, Apr 2024, https://arxiv.org/pdf/2403.20330.pdf

### HITL (Human in the Loop)
### LLM as Judge
---
## LLM Evaluation
### Embeddings
### In Context Learning
### Hallucinations
### Multi Turn
### Multi-Lingual
### Multi-Modal
### Instruction Following
### Ethical AI
### Safe AI
### Code Generating LLMs


---

## LLM Systems
### RAG Evaluation
### Conversational systems
### Copilots
### Search and Recommendation Engines
### Task Utility
#### Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications, Feb 2024, https://arxiv.org/abs/2402.09015


