# LLM Evaluation
# Compendium of LLM Evaluation methods

---
### Reviews and Surveys

### Leaderboards and Arenas
#### LMSys Arena https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
#### MTEB https://huggingface.co/spaces/mteb/leaderboard
#### SWE Bench https://www.swebench.com/
#### Gorilla, Berkeley function calling Leaderboard https://gorilla.cs.berkeley.edu/leaderboard.html https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html
#### WildBench WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild https://huggingface.co/spaces/allenai/WildBench
#### Enterprise Scenarios, Patronus https://huggingface.co/blog/leaderboard-patronus
#### Vectara Halluzinations https://github.com/vectara/hallucination-leaderboard

### Evaluation Software

### LLM Evaluation articles in tech media and blog posts from companies

### Large benchmarks

### Evaluation theory, evaluation methods
### LLM as Judge
---
## LLM Evaluation
### Embeddings
### In Context Learning
### Hallucinations
### Multi Turn
### Multi-Lingual
### Multi-Modal
### Instruction Following
### Ethical AI
### Safe AI
### Code Generating LLMs


---

## LLM Systems
### RAG Evaluation
### Conversational systems
### Copilots
### Search and Recommendation Engines


